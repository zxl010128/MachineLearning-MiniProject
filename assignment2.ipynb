{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# COMP3411/9418 21T0 Assignment 2\n",
    "\n",
    "- Lecturer: Anna Trofimova\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Last Update 26th January at 07:00am, 2021\n",
    "$$\n",
    "% macros\n",
    "\\newcommand{\\indep}{\\perp \\!\\!\\!\\perp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Imagine that you have been hired as a Data Scientist by Amazon, and your first task is to evaluate customer sentiment towards their products. Many online stores selling Amazon products provide their customers with an option to leave a review, but they might not have a rating system, or the customers might choose to leave a review without rating. However, you still want to use these reviews in your report. Thus, you need to develop a reliable model that can automatically assign setiment given a review.\n",
    "\n",
    "To develop your model, you have been given a collection of customer reviews on products like Alexa Echo, Echo dots, Alexa Firesticks etc. with their corresponding ratings. The ratings vary from 1 to 5, but to simplify the problem, you will consider reviews with ratings 1 & 2 to have negative sentiment, with 3 having neutral sentiment,  and 4 & 5 having positive sentiment.\n",
    "* Negative: 1 & 2\n",
    "* Neutral: 3\n",
    "* Positive: 4 & 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "When working with a Jupyter notebook, you can edit the \\*.py files either in the Jupyter interface (in your browser) or with your favorite editor (e.g., PyCharm). Whenever you save a \\*.py file, the notebook will reload their content directly.\n",
    "\n",
    "**Do not create new markdown cells, if you want to comment on something then use Raw NBConvert cells.**\n",
    "\n",
    "Below are the libraries that you can use (and need) in this assignment. If you want to use a library that is not in the list then send us an email to confirm (use course email). \n",
    "\n",
    "Run the code below to import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "#  If you experience problems with downloading stopwords, uncomment and run the code below.\n",
    "#  It will launch NLTK Downloader application so you can download stopwords corpora manually.\n",
    "\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data [2 marks]\n",
    "\n",
    "In this part of the assignment you need to import the Amazon reviews dataset and analyse its main properties.\n",
    "\n",
    "#### Task 1.1\n",
    "Import the dataset from *amazon_alexa.tsv* file, save it into the variable *data* and change the rating labels as follow: \n",
    " {1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "AmazonData = pd.read_csv (\"amazon_alexa.tsv\", sep = '\\t')\n",
    "AmazonData['rating'].replace({1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}, inplace=True)\n",
    "data = AmazonData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to plot the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['rating'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a simple description of the class frequency distribution across the full dataset. What do you notice about the distribution? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Across the full dataset, we can see there are 3 classes named 'postive', 'nagative' and 'neutral' respectively. As shown in the coloum graph, it has more than 2,500 positive rating, where as the nagative rating and neutral rating are less than 500. The number of negative rating is slightly higher then the number of neutral rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2\n",
    "\n",
    "To reduce computation time in this task, your will use only the 1000 most frequent tokens from the vocabulary as attributes. Run the code below to convert the input text samples into a document-term matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!!</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>&amp;#34;Alexa,</th>\n",
       "      <th>&amp;#34;Things</th>\n",
       "      <th>(which</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldn't</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet.</th>\n",
       "      <th>you</th>\n",
       "      <th>you're</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  !!  &  &#34;Alexa,  &#34;Things  (which  ,  -  --  .  ...  worth  would  \\\n",
       "0  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "1  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "2  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "3  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "4  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "\n",
       "   wouldn't  year  years  yet  yet.  you  you're  your  \n",
       "0         0     0      0    0     0    0       0     0  \n",
       "1         0     0      0    0     0    0       0     0  \n",
       "2         0     0      0    0     0    2       0     0  \n",
       "3         0     0      0    0     0    0       0     0  \n",
       "4         0     0      0    0     0    0       0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(data['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "x_sparse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was written in the helper notebook that the original input representation is bad. Explain why a document-term matrix is a more suitable representation for machine learning tasks with text data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# A document-term matrix is a mathematical matrix that describes the frequency of terms that occur in the original input. It helps us to pre-process the data, and count the times of terms that appear in each review. We could easily use the data provided by the matrix to do analysis. If we don't transform the original input into the document-term matrix, we need to do heavy work on preparation(such as loop all the reviews, count the number of each term and store them in a dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two variables *x_dense* and *x_sparce* both represent input data in a document-term form. Explain which one is preferable and why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I think x_sparce is preferable. The advantage of x_dense is that it ignores the 0 value as well as store the index instead of the terms. So the memory used is quite low and the processing time is faster. Although x_sparse doesn't store index, x_sparse is more visual, which means we can easily plot this data as a table. It is clear for users to use the data at the beginning. At the same time, the formed matrix also has the mathematical meaning, which means that it is easier to compute the data directly through python modules such as pandas. It is also helpful for the computer to analyse the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Decision Tree [2 marks]\n",
    "\n",
    "In this part you need to evaluate how classifier hyperparameters affect their performance. Consider a Decision Tree classifier that splits attributes based on the lowest entropy. For performance evaluation, assume that only the top 1000 most frequent tokens are used as attributes. The first 60% of samples should constitute the traing set while the remaining 40% of samples should constitute the test set.\n",
    "\n",
    "#### Task 2.1\n",
    "In the cell below place your code that computes performance measures for decision trees with different depth limits. <br> \n",
    "Consider the following cases: max_depth = 5, 10, 100, 200, None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "===================== Depth: 5 ====================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.07      0.12       108\n",
      "     neutral       0.07      0.01      0.02        67\n",
      "    positive       0.87      0.98      0.92      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.45      0.36      0.36      1260\n",
      "weighted avg       0.79      0.85      0.80      1260\n",
      "\n",
      "======================================================\n",
      "===================== Depth: 10 ====================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.32      0.09      0.14       108\n",
      "     neutral       0.00      0.00      0.00        67\n",
      "    positive       0.87      0.97      0.92      1085\n",
      "\n",
      "    accuracy                           0.84      1260\n",
      "   macro avg       0.40      0.35      0.35      1260\n",
      "weighted avg       0.78      0.84      0.80      1260\n",
      "\n",
      "======================================================\n",
      "===================== Depth: 100 ====================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.23      0.22      0.23       108\n",
      "     neutral       0.08      0.07      0.08        67\n",
      "    positive       0.89      0.90      0.90      1085\n",
      "\n",
      "    accuracy                           0.80      1260\n",
      "   macro avg       0.40      0.40      0.40      1260\n",
      "weighted avg       0.79      0.80      0.80      1260\n",
      "\n",
      "======================================================\n",
      "===================== Depth: 200 ====================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.22      0.23      0.23       108\n",
      "     neutral       0.07      0.06      0.06        67\n",
      "    positive       0.89      0.90      0.90      1085\n",
      "\n",
      "    accuracy                           0.80      1260\n",
      "   macro avg       0.40      0.40      0.40      1260\n",
      "weighted avg       0.79      0.80      0.79      1260\n",
      "\n",
      "======================================================\n",
      "===================== Depth: None ====================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.25      0.25       108\n",
      "     neutral       0.06      0.06      0.06        67\n",
      "    positive       0.89      0.89      0.89      1085\n",
      "\n",
      "    accuracy                           0.80      1260\n",
      "   macro avg       0.40      0.40      0.40      1260\n",
      "weighted avg       0.79      0.80      0.79      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, data['rating'], test_size = 0.4, shuffle=False)\n",
    "\n",
    "for depth in [5,10,100,200,None]:\n",
    "    \n",
    "    print('======================================================')\n",
    "    print(f'===================== Depth: {depth} ====================' + '\\n')\n",
    "\n",
    "    DTClassifier = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "    TrainingModel = DTClassifier.fit(X_train, Y_train)\n",
    "    YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2\n",
    "In the cell below explain any difference in performance, and comment on metrics in relation to the depth limit."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# As we can see from the table, the positive class only has a slight improvement on the precision, recall, and f1-score. The reason may include that the proportion of the positive class is really high. Although the depth is not high, it also can have more correctly predicted data. As for the negative class and neutral class, because they have relatively low percentages, the precision, recall, and f1-score are also lower when the depth is 5 and 10. Even the neutral class has a 0 precision and 0 recall when depth is 5. When the depth is higher than 100, we can see that there are several neutral data which has been predicted correctly, and the recall, and f1-score of the negative class also increases moderately. Therefore, it can be concluded that with the increment of depth, the precision, recall, and f1-score will also increase. However the accuracy seems to reduce from depth-10 to depth-100. The probable reason is that the max_depth is set too high. The decision tree becomes more complicated. Then the decision tree might simply overfit the training data without capturing useful patterns as we would like. Therefore, it will cause testing error to increase and accuracy to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Pre-processing [3 marks]\n",
    "\n",
    "In this part, you need to evaluate the effect that the pre-processing of input features has on the performance of three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and  Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part2 but do not limit the depth. For a Artificial Neural Network classifier, set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitutes the traing set while the remaining 40% of samples constitutes the test set. All tokens in the vocabulary should be used as attributes.\n",
    "Consider the following scenarios:\n",
    "1. No data pre-processing\n",
    "2. Removal of URLs and tokens that are not made of strings of letters, numbers, or symbols {’, #, @, $} delimited by spaces.\n",
    "3. Apply (2) and make all tokens lowercase.\n",
    "4. Apply (2), (3) and remove all stop words.\n",
    "\n",
    "#### Task 3.1\n",
    "In the cell below place your code that reflects the four scenarios of feature pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# case 1\n",
    "def preprocessing_1(x):\n",
    "    \n",
    "    processedData = x.copy()\n",
    "    return processedData\n",
    "\n",
    "# case 2\n",
    "def preprocessing_2(x):\n",
    "\n",
    "    processedData = x.copy()\n",
    "    processedData['verified_reviews'] = processedData['verified_reviews'].replace('http[s]?://\\S+', '', regex=True)\n",
    "    processedData['verified_reviews'] = processedData['verified_reviews'].replace(r'[^A-Za-z0-9#@$%]{2,}', ' ', regex=True)\n",
    "    processedData['verified_reviews'] = processedData['verified_reviews'].apply(lambda words: ' '.join(word for word in words.split()))    \n",
    "    return processedData\n",
    "\n",
    "# case 3\n",
    "def preprocessing_3(x):\n",
    "\n",
    "    processedData = x.copy()\n",
    "    dataAfterRemoval = preprocessing_2(processedData)\n",
    "    dataAfterRemoval['verified_reviews'] = dataAfterRemoval['verified_reviews'].str.lower()\n",
    "    dataAfterRemoval['verified_reviews'] = dataAfterRemoval['verified_reviews'].apply(lambda words: ' '.join(word for word in words.split())) \n",
    "    return dataAfterRemoval\n",
    "\n",
    "# case 4\n",
    "def preprocessing_4(x):\n",
    "    \n",
    "    stopwordsList = stopwords.words('english')\n",
    "    processedData = x.copy()\n",
    "\n",
    "    dataAfterLower = preprocessing_3(processedData)    \n",
    "    dataAfterLower['verified_reviews'] = dataAfterLower['verified_reviews'].apply(lambda words: ' '.join(word for word in words.split() if word not in stopwordsList))\n",
    "\n",
    "    return dataAfterLower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Pre-process your data according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "=============== DTClassifier Report ==================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.37      0.37       108\n",
      "     neutral       0.17      0.09      0.12        67\n",
      "    positive       0.90      0.93      0.92      1085\n",
      "\n",
      "    accuracy                           0.84      1260\n",
      "   macro avg       0.48      0.46      0.47      1260\n",
      "weighted avg       0.82      0.84      0.83      1260\n",
      "\n",
      "======================================================\n",
      "=============== MNBClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.37      0.47       108\n",
      "     neutral       0.22      0.16      0.19        67\n",
      "    positive       0.91      0.96      0.93      1085\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.58      0.50      0.53      1260\n",
      "weighted avg       0.85      0.87      0.85      1260\n",
      "\n",
      "======================================================\n",
      "=============== ANNClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.22      0.27       108\n",
      "     neutral       0.10      0.04      0.06        67\n",
      "    positive       0.90      0.96      0.93      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.45      0.41      0.42      1260\n",
      "weighted avg       0.81      0.85      0.82      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(preprocessing_4(data)['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, data['rating'], test_size = 0.4, shuffle=False)\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== DTClassifier Report ==================' + '\\n')\n",
    "\n",
    "DTClassifier = tree.DecisionTreeClassifier()\n",
    "TrainingModel = DTClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== MNBClassifier Report =================' + '\\n')\n",
    "\n",
    "MNBClassifier = MultinomialNB()\n",
    "TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== ANNClassifier Report =================' + '\\n')\n",
    "\n",
    "ANNClassifier = MLPClassifier(hidden_layer_sizes=(200,))\n",
    "TrainingModel = ANNClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3\n",
    "\n",
    "Evaluate the effect each pre-processing scenario has on the performance for the three models. In each case. compute the performance metrics. Write your answer in the cell below. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Across all these three classifiers, it is clear that after we do the preprocessing2, the accuracy has a slight improvement compared with no data pre-processing. For all three classes(positive, negative, neutral), the precision, recall, and f1-score all increase as well. After we make all tokens lowercase, the accuracy for the Decision Tree classifier goes up from 0.80 to 0.84, and the f1-score of the neutral class also increases dramatically, from 0.05 to 0.21. This shows that preprocessing3 is useful for decision tree classifier to improve accuracy. However, for Multinomial Naive Bayes classifier and Artificial Neural Networks classifier, it seems to be unchanged. The data provided by reports are close. After we remove stopwords from the reviews(proprecessing4), the accuracy for both Decision Tree classifier and Artificial Neural Networks classifier decreases. The accuracy for Multinomial Naive Bayes classifier remains unchanged. The reason may be varied. For example, A customer who gave a negative rating may leave a review \"I don't like it\". A customer who gave a positive rating may leave a review \"I like it\". After we removed the stopwords, these two reviews only contained \"like\". Therefore, this situation will cause the wrong predicted rate increases.\n",
    "  In conclusion, we can conclude that preprocessing2 and preprocessing3 help machines to increase the accuracy of training, whereas preprocessing4 is not a good method. When we do the same preprocessing, it should also be mentioned that Multinomial Naive Bayes classifier performs better than other two classifiers with higher accuracy and higher correctly predicted rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 [5 marks]\n",
    "\n",
    "In this part, you need to evaluate how the number of features and classes affects the performance of the three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part 2 but do not limit the depth. For a Artificial Neural Network classifier set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1\n",
    "\n",
    "Consider the following scenarios for attributes:\n",
    "\n",
    "1. Attributes are all tokens in the vocabulary\n",
    "2. Attributes are the top 1000 most frequent tokens in the vocabulary\n",
    "3. Attributes are the top 100 most frequent tokens in the vocabulary\n",
    "4. Attributes are the top 10 most frequent tokens in the vocabulary\n",
    "\n",
    "In the cell below, place your code that returns a document-term representation given the number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "def limit_attributes(x, n_attributes):\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=n_attributes)\n",
    "\n",
    "    x_dense = vectorizer.fit_transform(x['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "    return x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Limit the number of attributes according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "=============== DTClassifier Report ==================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.17      0.14      0.15       108\n",
      "     neutral       0.07      0.06      0.06        67\n",
      "    positive       0.88      0.90      0.89      1085\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.37      0.37      0.37      1260\n",
      "weighted avg       0.77      0.79      0.78      1260\n",
      "\n",
      "======================================================\n",
      "=============== MNBClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.02      0.04       108\n",
      "     neutral       0.00      0.00      0.00        67\n",
      "    positive       0.86      1.00      0.93      1085\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.62      0.34      0.32      1260\n",
      "weighted avg       0.83      0.86      0.80      1260\n",
      "\n",
      "======================================================\n",
      "=============== ANNClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.06      0.10       108\n",
      "     neutral       0.07      0.01      0.02        67\n",
      "    positive       0.87      0.99      0.92      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.44      0.35      0.35      1260\n",
      "weighted avg       0.79      0.85      0.81      1260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangxinlei/Library/Python/3.8/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "x_sparse = limit_attributes(data, 10)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, data['rating'], test_size = 0.4, shuffle=False)\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== DTClassifier Report ==================' + '\\n')\n",
    "\n",
    "DTClassifier = tree.DecisionTreeClassifier()\n",
    "TrainingModel = DTClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== MNBClassifier Report =================' + '\\n')\n",
    "\n",
    "MNBClassifier = MultinomialNB()\n",
    "TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== ANNClassifier Report =================' + '\\n')\n",
    "\n",
    "ANNClassifier = MLPClassifier(hidden_layer_sizes=(200,))\n",
    "TrainingModel = ANNClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3\n",
    "\n",
    "Evaluate the effect each scenario has on the performance for the three models based on the performance metrics. Explain the differences in the performance. Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# According to the performance reports, the scenario 1 has the best performance(no limit on the number of frequent tokens). All three classifiers have a 0.85 accuracy at least. When we strat to restrict the number of frequent tokens (scenario 2), the accuracy for Decision Tree classifier goes down moderately from 0.85 to 0.80, whereas the accuracy for Multinomial Naive Bayes classifier and Artificial Neural Networks classifier stay the same or slightly reduce. When the restriction becomes more strict (scenario 3 and scenario 4), the accuracy only has slight changes across all three classifiers(around 0,1 reduction). But for the precision, recall, and f1-score, we could see that the positive class is still unchanged, whereas the negative class has a decreasing sharply. For the neutral class, the data seems to fluctuate. The reason why accuracy does not reduce too much might be the proportion of positive reviews is much higher than the proportion of negative reviews and neutral reviews. The reduction on negative class and neutral class only have a slight influence on the accuracy of the model. As for the neutral class, the reason why the value seems to fluctuate may be the sample size is really rare, therefore the result will be less accurate.\n",
    "  In conclusion, we can see that the restriction of frequent tokens does have an impact on the accuracy of the model. The less number the restriction has, the less accurate the model has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.4\n",
    "\n",
    "Remove all samples with neutral sentiment from the dataset. Train and test all three models on this new dataset, computing the preformance measures as well. Assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set. All tokens in the vocabulary should be used as attributes.\n",
    "\n",
    "Place your code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "=============== DTClassifier Report ==================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.33      0.17      0.23       109\n",
      "    positive       0.92      0.97      0.94      1091\n",
      "\n",
      "    accuracy                           0.89      1200\n",
      "   macro avg       0.63      0.57      0.59      1200\n",
      "weighted avg       0.87      0.89      0.88      1200\n",
      "\n",
      "======================================================\n",
      "=============== MNBClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.12      0.18       109\n",
      "    positive       0.92      0.98      0.95      1091\n",
      "\n",
      "    accuracy                           0.90      1200\n",
      "   macro avg       0.63      0.55      0.56      1200\n",
      "weighted avg       0.87      0.90      0.88      1200\n",
      "\n",
      "======================================================\n",
      "=============== ANNClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.28      0.40       109\n",
      "    positive       0.93      0.99      0.96      1091\n",
      "\n",
      "    accuracy                           0.92      1200\n",
      "   macro avg       0.81      0.64      0.68      1200\n",
      "weighted avg       0.91      0.92      0.91      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "def processingRemoveNeural(x):\n",
    "    \n",
    "    frameData = x.copy()\n",
    "\n",
    "    frameData.drop(frameData[frameData['rating'] == 'neutral'].index, inplace=True)\n",
    "    \n",
    "    frameData.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return frameData\n",
    "\n",
    "\n",
    "dataProcessed = processingRemoveNeural(data)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split())\n",
    "\n",
    "x_dense = vectorizer.fit_transform(dataProcessed['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, dataProcessed['rating'], test_size = 0.4, shuffle=False)\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== DTClassifier Report ==================' + '\\n')\n",
    "\n",
    "DTClassifier = tree.DecisionTreeClassifier()\n",
    "TrainingModel = DTClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== MNBClassifier Report =================' + '\\n')\n",
    "\n",
    "MNBClassifier = MultinomialNB()\n",
    "TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== ANNClassifier Report =================' + '\\n')\n",
    "\n",
    "ANNClassifier = MLPClassifier(hidden_layer_sizes=(200,))\n",
    "TrainingModel = ANNClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.5\n",
    "Compare these results to the results obtained in scenario #1 (part 4). Is there any difference in the metrics for either of the classes (i.e. consider positive and negative classes individually)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# After we remove the neutral class, we can see that the accuracies for these three classifiers are all increase dramatically, and all of these have reached 0.89. More specifically, for the positive class and negative class, the value of precision, recall, and f1-score increase obviously as well, which means that the correctly prediected cases increases in Y_test and Y_predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 [8 marks]\n",
    "\n",
    "In this part you need to develop your own method (pipeline) for sentiment analysis. You can create new code cells (to place your code) and Raw NBConvert cells (to descibe the steps you are taking to develop your model, or make a comment).\n",
    "\n",
    "You can use either Decision Tree, Multinomial Naive Bayes (MNB), or Artificial Neural Network (ANN) classifiers. If you use parameters for your classifier, the you need to justify the values that you chose for them in writing or in coding & writing (if it is based on evaluation resuts) - similar to Part 2.\n",
    "\n",
    "Since this part of the assignment carries the highest mark, the explanation of your method and its evaluation should be descibed in detail.\n",
    "\n",
    "Optionally: you can also make plots to visualize your findings. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# My first idea is that I will not remove the neutral class from the data. Because I think the neutral rating is also the representation of part of customers' attitude. Although the proportion of the neutral rating is low, which causes that the predicted result is not quite accurate, we cannot ignore this rating which is also crucial for Amazon to analyze their goods. We should try to figure out how to increase the accuracy instead of ignoring it.\n",
    "\n",
    "# I decide to use Multinomial Naive Bayes as the classifier. As the data shown above, in part 3 and part 4(except 4.4), Multinomial Naive Bayes classifier usually has higher accuracy than Decision Tree Classifier and Artificial Neural Network classifier. Although sometimes the accuracy of ANN classifier is higher then MNB classifier, the predicted rate is also higher for MNB classifier when I check the value of precision, recall, and f1-score."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In part3, we can see that the preprocessing3 helps a lot to increase the accuracy of the model. Therefore, I decide to continue working on pre-processing based on preprocessing3. Below are the code and report of the training model by using preprocessing3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "=============== MNBClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.12      0.20       108\n",
      "     neutral       0.20      0.03      0.05        67\n",
      "    positive       0.87      0.99      0.93      1085\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.55      0.38      0.39      1260\n",
      "weighted avg       0.81      0.86      0.82      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split())\n",
    "\n",
    "processedData = preprocessing_3(data)\n",
    "x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = 0.4, shuffle=False)\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== MNBClassifier Report =================' + '\\n')\n",
    "\n",
    "MNBClassifier = MultinomialNB()\n",
    "TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The next thing I consider is to tokenize the words. It is similar to the split function in python, however, the tokenization function provided by nltk seems to be more professional and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "=============== MNBClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.15      0.24       108\n",
      "     neutral       0.22      0.03      0.05        67\n",
      "    positive       0.87      0.99      0.93      1085\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.59      0.39      0.41      1260\n",
      "weighted avg       0.82      0.87      0.82      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def preprocessingText(x):\n",
    "    \n",
    "    processedData = x.copy()\n",
    "    processedData = preprocessing_3(processedData)\n",
    "    processedData['verified_reviews'] = processedData['verified_reviews'].apply(word_tokenize) \n",
    "    return processedData\n",
    "\n",
    "processedData = preprocessingText(data)\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = 0.4, shuffle=False)\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== MNBClassifier Report =================' + '\\n')\n",
    "\n",
    "MNBClassifier = MultinomialNB()\n",
    "TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# As we can see, the accuracy goes up from 0.86 to 0.87. As for each class, the value of precision, recall, and f1-score also increases. Therefore, tokenization is more helpful compared with the split function.\n",
    "\n",
    "# What's more, removing stopwords is a way to increase accuracy in my opinion. However, some stopwords in the nltk list might have a negative impact on the accuracy. So, I decide to select some stopwords by myself that definitely have no impact on the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyStopwordsList = ['i', 'we', 'our', 'you', 'he', 'his', 'she', 'how', 'where', 'why', 'which', 'when'\n",
    "                    , 'her', 'it', 'its', 'they', 'their','themselves', 'that', 'ok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "=============== MNBClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.17      0.28       108\n",
      "     neutral       0.20      0.03      0.05        67\n",
      "    positive       0.88      0.99      0.93      1085\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.63      0.40      0.42      1260\n",
      "weighted avg       0.84      0.87      0.83      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocessingStopWords(x):\n",
    "\n",
    "    processedData = preprocessingText(x).copy()\n",
    "    processedData['verified_reviews'] = processedData['verified_reviews'].apply(lambda words: ' '.join(word for word in words if word not in MyStopwordsList))\n",
    "    processedData['verified_reviews'] = processedData['verified_reviews'].apply(word_tokenize) \n",
    "    return processedData\n",
    "\n",
    "processedData = preprocessingStopWords(data)\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = 0.4, shuffle=False)\n",
    "\n",
    "\n",
    "print('======================================================')\n",
    "print('=============== MNBClassifier Report =================' + '\\n')\n",
    "\n",
    "MNBClassifier = MultinomialNB()\n",
    "TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Although the total accuracy is unchanged, there are more cases that are predicted correctly for positive class and negative class.\n",
    "\n",
    "# I think it is all for data pre-processing. Based on the preprocessing3, I add two more methods to increase the accuracy of the model. The first one is to replace the python split function with the tokenization function provided by nltk which is more professional and suitable. And then I create a new stopwords list that is more suitable to fit the circumstance.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Then, the parameters of train_test_split function are also needed to consider. The first thing I think is the test_size might has an impact on the accuracy. Because there is a huge difference between the proportion of the positive class and the proportion of the negative class and neutral class. We need to find an appropriate test_size to fit this situation. Shuffle may also influence the result of the model. We also need to fix the dataset, because shuffle could make the result fluctuate. Therefore, I set random_state to 42 which is a famous number, and try to change test_size and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "testsizes = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "accuracyList = []\n",
    "\n",
    "processedData = preprocessingStopWords(data)\n",
    "\n",
    "for testSize in testsizes:\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "    x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = testSize, shuffle=False, random_state=42)\n",
    "\n",
    "    MNBClassifier = MultinomialNB()\n",
    "    TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "    YModelTest = TrainingModel.predict(X_test)\n",
    "    accuracyList.append(round(classification_report(Y_test, YModelTest,output_dict=True).get('accuracy'), 2))\n",
    "\n",
    "plt.plot(testsizes, accuracyList, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# As we can see from the diagram when the test sizes are 0.05, 0.10, 0.15, and 0.3 and shuffle is false, the accuracy for these situations all reaches 0.89. We extract these four sizes and re-run them again with the shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "testsizes = [0.05, 0.1, 0.15, 0.3]\n",
    "accuracyList = []\n",
    "\n",
    "processedData = preprocessingStopWords(data)\n",
    "\n",
    "for testSize in testsizes:\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "    x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = testSize, random_state=42)\n",
    "\n",
    "    MNBClassifier = MultinomialNB()\n",
    "    TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "    YModelTest = TrainingModel.predict(X_test)\n",
    "    accuracyList.append(round(classification_report(Y_test, YModelTest,output_dict=True).get('accuracy'), 2))\n",
    "\n",
    "plt.plot(testsizes, accuracyList, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Shuffle does help improve the accuracy of the model. After that, I will print the report with 0.05 test size and 0.1 test size in order to select the best test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== MNBClassifier Report testSize: 0.05 =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.53      0.64        15\n",
      "     neutral       0.00      0.00      0.00         8\n",
      "    positive       0.92      0.99      0.95       135\n",
      "\n",
      "    accuracy                           0.90       158\n",
      "   macro avg       0.57      0.51      0.53       158\n",
      "weighted avg       0.86      0.90      0.88       158\n",
      "\n",
      "=============== MNBClassifier Report testSize: 0.1 =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.34      0.47        29\n",
      "     neutral       0.50      0.07      0.12        14\n",
      "    positive       0.91      1.00      0.95       272\n",
      "\n",
      "    accuracy                           0.90       315\n",
      "   macro avg       0.71      0.47      0.51       315\n",
      "weighted avg       0.87      0.90      0.87       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testsizes = [0.05, 0.1]\n",
    "\n",
    "processedData = preprocessingStopWords(data)\n",
    "\n",
    "for testSize in testsizes:\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "    x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = testSize, random_state=42)\n",
    "\n",
    "    print(f'=============== MNBClassifier Report testSize: {testSize} =================' + '\\n')\n",
    "\n",
    "    MNBClassifier = MultinomialNB()\n",
    "    TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "    YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We could see that these two sizes with shuffle both have 0.9 accuracies. However, neutral cases have not been predicted in the first model. The reason might be that there are rare cases in the test dataset. Therefore, 0.1 is the best choice.\n",
    "\n",
    "# I also consider adding a \"feedback\" column into the model. After I added it into my model, all the negative cases could be predicted correctly, which shows that the accuracy exceeded 0.95. However, after I carefully read the spec, I found that the \"feedback\" column is calculated from the \"rating\" column. Our purpose is to develop a reliable model that can automatically assign rating given a review. This means that the \"feedback\" column does not exist when there is no rating. Therefore, we could not use this data in the training model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The last thing I would like to change is the parameters of the classifier. I tried to set fit_prior to False and found that the accuracy dropped dramatically. It means that learning class prior probabilities will help improve accuracy. After that, I tried to modify the alpha value which is the additive smoothing parameter. Some value definitely helped improve accuracy from 0.9 to 0.91. Below are my code and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== MNBClassifier Report Alpha: 0.5 =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.48      0.61        29\n",
      "     neutral       0.56      0.36      0.43        14\n",
      "    positive       0.93      0.99      0.96       272\n",
      "\n",
      "    accuracy                           0.91       315\n",
      "   macro avg       0.77      0.61      0.67       315\n",
      "weighted avg       0.90      0.91      0.90       315\n",
      "\n",
      "=============== MNBClassifier Report Alpha: 0.6 =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.48      0.61        29\n",
      "     neutral       0.50      0.29      0.36        14\n",
      "    positive       0.93      0.99      0.96       272\n",
      "\n",
      "    accuracy                           0.91       315\n",
      "   macro avg       0.75      0.59      0.64       315\n",
      "weighted avg       0.90      0.91      0.90       315\n",
      "\n",
      "=============== MNBClassifier Report Alpha: 0.7 =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.48      0.61        29\n",
      "     neutral       0.57      0.29      0.38        14\n",
      "    positive       0.92      0.99      0.96       272\n",
      "\n",
      "    accuracy                           0.91       315\n",
      "   macro avg       0.77      0.59      0.65       315\n",
      "weighted avg       0.90      0.91      0.90       315\n",
      "\n",
      "=============== MNBClassifier Report Alpha: 0.8 =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.48      0.62        29\n",
      "     neutral       0.60      0.21      0.32        14\n",
      "    positive       0.92      0.99      0.95       272\n",
      "\n",
      "    accuracy                           0.91       315\n",
      "   macro avg       0.80      0.56      0.63       315\n",
      "weighted avg       0.90      0.91      0.90       315\n",
      "\n",
      "=============== MNBClassifier Report Alpha: 0.9 =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.48      0.60        29\n",
      "     neutral       0.50      0.07      0.12        14\n",
      "    positive       0.92      1.00      0.96       272\n",
      "\n",
      "    accuracy                           0.91       315\n",
      "   macro avg       0.73      0.52      0.56       315\n",
      "weighted avg       0.89      0.91      0.89       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for alpha in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    \n",
    "    processedData = preprocessingStopWords(data)\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "    x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = 0.1, random_state=42)\n",
    "\n",
    "    print(f'=============== MNBClassifier Report Alpha: {alpha} =================' + '\\n')\n",
    "\n",
    "    MNBClassifier = MultinomialNB(alpha = alpha)\n",
    "    TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "    YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# As shown above, these 5 alpha values could improve the accuracy from 0.9 to 0.91. The best one is when the alpha is 0.5. It has the best f1-score for all three classes across these 5 alpha values.\n",
    "\n",
    "# Therefore my new model has been created. I did 2 more pre-processing of the reviews, modified the test_size and shuffle situation, and modified the alpha value for the classifier. Below is the complete code of my new mode. The preprocessing function is written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== MNBClassifier Report =================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.48      0.61        29\n",
      "     neutral       0.56      0.36      0.43        14\n",
      "    positive       0.93      0.99      0.96       272\n",
      "\n",
      "    accuracy                           0.91       315\n",
      "   macro avg       0.77      0.61      0.67       315\n",
      "weighted avg       0.90      0.91      0.90       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processedData = preprocessingStopWords(data)\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(processedData['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_sparse, processedData['rating'], test_size = 0.1, random_state=42)\n",
    "\n",
    "print(f'=============== MNBClassifier Report =================' + '\\n')\n",
    "\n",
    "MNBClassifier = MultinomialNB(alpha = 0.5)\n",
    "TrainingModel = MNBClassifier.fit(X_train, Y_train)\n",
    "YModelTest = TrainingModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, YModelTest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Let's compare the original model and the new model performance. Below are the reports.\n",
    "\n",
    "======================================================\n",
    "========== MNBClassifier Original Report =============\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    negative       0.59      0.12      0.20       108\n",
    "     neutral       0.20      0.03      0.05        67\n",
    "    positive       0.87      0.99      0.93      1085\n",
    "\n",
    "    accuracy                           0.86      1260\n",
    "   macro avg       0.55      0.38      0.39      1260\n",
    "weighted avg       0.81      0.86      0.82      1260\n",
    "\n",
    "======================================================\n",
    "============ MNBClassifier New Report ================\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    negative       0.82      0.48      0.61        29\n",
    "     neutral       0.56      0.36      0.43        14\n",
    "    positive       0.93      0.99      0.96       272\n",
    "\n",
    "    accuracy                           0.91       315\n",
    "   macro avg       0.77      0.61      0.67       315\n",
    "weighted avg       0.90      0.91      0.90       315"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In conclusion, the new model has a big improvement in accuracy compared with the original model. The values of precision, recall, and f1-score increase dramatically as well across all the three classes(positive, negative, neutral)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
